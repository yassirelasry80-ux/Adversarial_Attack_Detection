import torch
from PIL import Image
import matplotlib.pyplot as plt
from torchvision import transforms
from app.config import Config
from app.models.classifier import get_model, get_poison_detector
from app.models.detector import PoisonDetector
import os

class InferenceSystem:
    """
    Syst√®me d'inf√©rence pour d√©tecter les attaques et classifier les images
    """
    
    def __init__(self, model_path=None, detector_path=None, detector_type="supervised"):
        # D√©terminer le chemin du mod√®le global selon la m√©thode
        if model_path is None:
            if detector_type == "supervised":
                model_path = "global_model_supervised.pth"
            else:
                model_path = "global_model_autoencoder.pth"
        
        # Charger le mod√®le de classification
        self.model = get_model(pretrained=False)
        if os.path.exists(model_path):
            self.model.load_state_dict(torch.load(model_path, map_location=Config.DEVICE))
            print(f"‚úì Mod√®le global charg√© depuis {model_path} ({detector_type})")
        else:
            print(f"‚ö†Ô∏è Fichier {model_path} non trouv√©, utilisation du mod√®le pr√©-entra√Æn√©")
            # Essayons le fallback sur le nom g√©n√©rique
            if os.path.exists("global_model_final.pth"):
                self.model.load_state_dict(torch.load("global_model_final.pth", map_location=Config.DEVICE))
                print(f"‚ö†Ô∏è Replis sur global_model_final.pth")
        
        self.model.eval()
        
        # Charger le d√©tecteur d'attaques
        self.detector_type = detector_type
        
        if detector_type == "supervised":
            from app.models.detector import PoisonDetector
            self.detector = PoisonDetector(self.model)
            default_path = "poison_detector_best.pth"
        else:
            from app.models.detector import AutoEncoderDetector
            self.detector = AutoEncoderDetector()
            default_path = "autoencoder_best.pth"
            
        # D√©terminer le chemin du fichier
        final_path = detector_path if detector_path else default_path
        
        if os.path.exists(final_path):
            self.detector.load_detector(final_path)
            print(f"‚úì D√©tecteur ({detector_type}) charg√© depuis {final_path}")
        else:
            print(f"‚ö†Ô∏è D√©tecteur non trouv√©: {final_path}")

        # Transformation pour les images
        self.transform = transforms.Compose([
            transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        self.classes = ['NORMAL', 'PNEUMONIA']
    
    def predict_single_image(self, image_path, check_adversarial=True):
        """
        Pr√©dire la classe d'une seule image et d√©tecter les attaques
        
        Args:
            image_path: Chemin vers l'image
            check_adversarial: V√©rifier si l'image est adversariale
        
        Returns:
            dict avec la pr√©diction et les d√©tections
        """
        # Charger et pr√©traiter l'image
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.transform(image).unsqueeze(0).to(Config.DEVICE)
        
        results = {
            'image_path': image_path,
            'is_adversarial': False,
            'adversarial_confidence': 0.0,
            'prediction': None,
            'confidence': 0.0,
            'all_probabilities': {}
        }
        
        # V√©rifier si l'image est adversariale
        if check_adversarial:
            # detect_poison retourne (is_poisoned, output_value)
            # Pour supervis√©: output_value = probabilit√© (0-1)
            # Pour autoencodeur: output_value = erreur de reconstruction (MSE)
            is_poisoned, output_val = self.detector.detect_poison(image_tensor)
            
            results['is_adversarial'] = bool(is_poisoned.item())
            
            # Normalisation du score pour l'affichage
            if self.detector_type == "supervised":
                results['adversarial_confidence'] = float(output_val.item())
            else:
                # Pour l'AE, l'output_val est l'erreur MSE. 
                # On ne peut pas la convertir facilement en "confiance %", donc on garde la valeur brute
                # ou on l'affiche diff√©remment. Ici on met juste l'erreur.
                results['adversarial_confidence'] = float(output_val.mean().item())
        
        # Faire la pr√©diction
        with torch.no_grad():
            outputs = self.model(image_tensor)
            probabilities = torch.softmax(outputs, dim=1)[0]
            predicted_class = torch.argmax(probabilities).item()
            confidence = probabilities[predicted_class].item()
        
        results['prediction'] = self.classes[predicted_class]
        results['confidence'] = confidence
        results['all_probabilities'] = {
            self.classes[i]: float(probabilities[i].item()) 
            for i in range(len(self.classes))
        }
        
        return results
    
    def visualize_prediction(self, image_path, results):
        """
        Visualiser l'image avec la pr√©diction et la d√©tection d'attaque
        """
        # Charger l'image
        image = Image.open(image_path)
        
        # Cr√©er la figure
        fig, ax = plt.subplots(1, 1, figsize=(10, 8))
        ax.imshow(image, cmap='gray')
        ax.axis('off')
        
        # Titre avec la pr√©diction
        title = f"Pr√©diction: {results['prediction']} ({results['confidence']*100:.2f}%)\n"
        
        # Ajouter l'information sur l'attaque
        if results['is_adversarial']:
            title += f"‚ö†Ô∏è ATTAQUE D√âTECT√âE (confiance: {results['adversarial_confidence']*100:.2f}%)"
            title_color = 'red'
        else:
            title += f"‚úì Image propre (confiance: {results['adversarial_confidence']*100:.2f}%)"
            title_color = 'green'
        
        ax.set_title(title, fontsize=14, fontweight='bold', color=title_color)
        
        # Afficher les probabilit√©s
        prob_text = "Probabilit√©s:\n"
        for class_name, prob in results['all_probabilities'].items():
            prob_text += f"  {class_name}: {prob*100:.2f}%\n"
        
        plt.figtext(0.15, 0.02, prob_text, fontsize=10, ha='left')
        
        plt.tight_layout()
        plt.show()
    
    def batch_predict(self, image_paths):
        """
        Pr√©dire sur un lot d'images
        """
        results = []
        
        print(f"\nüîç Analyse de {len(image_paths)} images...\n")
        
        for i, image_path in enumerate(image_paths):
            print(f"[{i+1}/{len(image_paths)}] {os.path.basename(image_path)}")
            
            result = self.predict_single_image(image_path)
            results.append(result)
            
            # Afficher le r√©sultat
            status = "‚ö†Ô∏è ATTAQUE" if result['is_adversarial'] else "‚úì PROPRE"
            print(f"  {status} | Pr√©diction: {result['prediction']} ({result['confidence']*100:.2f}%)")
            print()
        
        return results

def demo():
    """
    D√©monstration du syst√®me d'inf√©rence
    """
    print("="*70)
    print("  üîç SYST√àME DE D√âTECTION ET CLASSIFICATION")
    print("="*70)
    
    # Cr√©er le syst√®me d'inf√©rence
    inference = InferenceSystem()
    
    # Exemple d'utilisation
    test_images_dir = os.path.join(Config.DATASET_PATH, "test", "NORMAL")
    
    if os.path.exists(test_images_dir):
        # Prendre quelques images de test
        image_files = [
            os.path.join(test_images_dir, f) 
            for f in os.listdir(test_images_dir)[:5]
            if f.endswith(('.jpeg', '.jpg', '.png'))
        ]
        
        # Pr√©dire sur ces images
        results = inference.batch_predict(image_files)
        
        # Afficher les statistiques
        num_adversarial = sum(1 for r in results if r['is_adversarial'])
        print("\n" + "="*70)
        print("R√âSUM√â:")
        print(f"  Images analys√©es: {len(results)}")
        print(f"  Attaques d√©tect√©es: {num_adversarial}")
        print(f"  Images propres: {len(results) - num_adversarial}")
        print("="*70)
    else:
        print(f"\n‚ö†Ô∏è R√©pertoire {test_images_dir} non trouv√©")
        print("Utilisez cette classe dans votre code:")
        print("\n  inference = InferenceSystem()")
        print("  result = inference.predict_single_image('path/to/image.jpg')") 
        print("  inference.visualize_prediction('path/to/image.jpg', result)")

if __name__ == "__main__":
    demo()
