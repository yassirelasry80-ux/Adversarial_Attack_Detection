import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import copy
from app.config import Config

class FederatedLearning:
    """
    Impl√©mentation de l'apprentissage f√©d√©r√©
    """
    
    def __init__(self, global_model):
        self.global_model = global_model
        self.local_models = []
    
    def train_local_model(self, model, dataloader, epochs=5):
        """
        Entra√Æner un mod√®le local sur les donn√©es d'un h√¥pital
        
        Args:
            model: Mod√®le local
            dataloader: DataLoader pour cet h√¥pital
            epochs: Nombre d'√©poques
        
        Returns:
            Mod√®le entra√Æn√©
        """
        model.train()
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)
        
        for epoch in range(epochs):
            total_loss = 0
            correct = 0
            total = 0
            
            for images, labels in dataloader:
                images = images.to(Config.DEVICE)
                labels = labels.to(Config.DEVICE)
                
                # Forward pass
                outputs = model(images)
                loss = criterion(outputs, labels)
                
                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                # Statistiques
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
                # Mise √† jour de la barre de progression (tqdm) serait top, mais on va juste print √† la fin
            
            accuracy = 100. * correct / total
            print(f"      Epoch {epoch+1}: Loss = {total_loss/len(dataloader):.4f}, Acc = {accuracy:.2f}%")
        
        return model
    
    def aggregate_models(self, local_models):
        """
        Agr√©gation FedAvg: moyenne des poids des mod√®les locaux
        
        Args:
            local_models: Liste des mod√®les locaux entra√Æn√©s
        """
        global_dict = self.global_model.state_dict()
        
        for key in global_dict.keys():
            # Calculer la moyenne des poids
            global_dict[key] = torch.stack([
                local_models[i].state_dict()[key].float() 
                for i in range(len(local_models))
            ]).mean(0)
        
        self.global_model.load_state_dict(global_dict)
    
    def federated_training(self, hospital_datasets, test_loader=None, num_rounds=Config.FEDERATED_ROUNDS):
        """
        Entra√Ænement f√©d√©r√© complet
        
        Args:
            hospital_datasets: Liste des datasets pour chaque h√¥pital
            test_loader: DataLoader pour √©valuer le mod√®le global √† chaque round
            num_rounds: Nombre de rounds f√©d√©r√©s
        """
        print(f"\nüè• D√©marrage de l'apprentissage f√©d√©r√© avec {len(hospital_datasets)} h√¥pitaux")
        print(f"   Nombre de rounds: {num_rounds}\n")
        
        for round_num in range(num_rounds):
            print(f"\n{'='*60}")
            print(f"ROUND F√âD√âR√â {round_num + 1}/{num_rounds}")
            print(f"{'='*60}")
            
            local_models = []
            
            # Entra√Æner chaque mod√®le local
            for hospital_id, dataset in enumerate(hospital_datasets):
                print(f"\nüè• H√¥pital {hospital_id + 1}/{len(hospital_datasets)}")
                print(f"   Taille du dataset: {len(dataset)} images")
                
                # Cr√©er un DataLoader pour cet h√¥pital
                dataloader = DataLoader(
                    dataset,
                    batch_size=Config.BATCH_SIZE,
                    shuffle=True,
                    num_workers=2,
                    pin_memory=True
                )
                
                # Copier le mod√®le global
                local_model = copy.deepcopy(self.global_model)
                
                # Entra√Æner localement
                print("   Entra√Ænement local...")
                local_model = self.train_local_model(
                    local_model, 
                    dataloader, 
                    epochs=Config.LOCAL_EPOCHS
                )
                
                local_models.append(local_model)
                print("   ‚úì Entra√Ænement local termin√©")
            
            # Agr√©ger les mod√®les
            print(f"\nüîÑ Agr√©gation des {len(local_models)} mod√®les locaux...")
            self.aggregate_models(local_models)
            print("‚úì Agr√©gation termin√©e")
            
            # √âvaluer le mod√®le global (et logging)
            if test_loader:
                print(f"\nüìä √âvaluation Round {round_num + 1}:")
                self.evaluate_global_model(test_loader)
            
            print(f"\n‚úì Round {round_num + 1} termin√©")
        
        print(f"\n{'='*60}")
        print("‚úì APPRENTISSAGE F√âD√âR√â TERMIN√â")
        print(f"{'='*60}\n")
        
        return self.global_model
    
    def evaluate_global_model(self, test_loader):
        """
        √âvaluer le mod√®le global sur le dataset de test
        
        Args:
            test_loader: DataLoader de test
        
        Returns:
            Accuracy du mod√®le
        """
        self.global_model.eval()
        correct = 0
        total = 0
        
        print("\nüìä √âvaluation du mod√®le global...")
        
        with torch.no_grad():
            for images, labels in tqdm(test_loader, desc="√âvaluation"):
                images = images.to(Config.DEVICE)
                labels = labels.to(Config.DEVICE)
                
                outputs = self.global_model(images)
                _, predicted = outputs.max(1)
                
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        accuracy = 100. * correct / total
        print(f"\n‚úì Accuracy du mod√®le global: {accuracy:.2f}%")
        
        return accuracy
    
    def save_global_model(self, path="global_model.pth"):
        """Sauvegarder le mod√®le global"""
        torch.save(self.global_model.state_dict(), path)
        print(f"‚úì Mod√®le global sauvegard√© dans {path}")
    
    def load_global_model(self, path="global_model.pth"):
        """Charger un mod√®le global"""
        self.global_model.load_state_dict(torch.load(path, map_location=Config.DEVICE))
        print(f"‚úì Mod√®le global charg√© depuis {path}")